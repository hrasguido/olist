# src/gold_fact_sales.py
"""
Capa Gold - Master Table Unificada para ML
Carga tablas desde Silver y construye master table con todos los datos
"""
from prefect import flow, task, get_run_logger
from sqlalchemy import create_engine
import pandas as pd
import numpy as np
import os
from dotenv import load_dotenv
from typing import Dict
from sklearn.feature_selection import mutual_info_regression, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
import optuna
from optuna.samplers import TPESampler
from model_evaluation import evaluate_models_with_cv
import pickle
from typing import List


from gold_features import (
    generate_temporal_features,
    generate_logistics_features,
    generate_payment_features,
    generate_customer_features,
    combine_features
)

load_dotenv("/workspace/.env")

SILVER_CONN = f"postgresql://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@db:5432/silver"
GOLD_CONN   = f"postgresql://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@db:5432/gold"

engine_silver = create_engine(SILVER_CONN)
engine_gold   = create_engine(GOLD_CONN)


@task(log_prints=True)
def load_datasets_from_silver() -> Dict[str, pd.DataFrame]:
    """Carga todos los datasets desde Silver."""
    logger = get_run_logger()
    logger.info("üì• Cargando datasets desde Silver...")
    
    datasets = {}
    tables = {
        'customers': 'customers',
        'geolocation': 'geolocation',
        'order_items': 'order_items',
        'order_payments': 'order_payments',
        'order_reviews': 'order_reviews',
        'orders': 'orders',
        'products': 'products',
        'sellers': 'sellers',
        'category_translation': 'product_category_translation'
    }
    
    for key, table in tables.items():
        try:
            logger.info(f"   Cargando: {table}")
            df = pd.read_sql(f"SELECT * FROM curated.{table}", engine_silver)
            datasets[key] = df
            logger.info(f"   ‚úÖ {key}: {len(df):,} registros")
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Error cargando {table}: {str(e)}")
            datasets[key] = pd.DataFrame()
    
    return datasets


@task(log_prints=True)
def build_master_table(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    Construye la master table unificada desde todas las fuentes.
    Replica la estructura de carga desde CSVs.
    """
    logger = get_run_logger()
    logger.info("üî® Construyendo Master Table en Gold...")
    
    # Verificar datasets requeridos
    required = ['orders', 'order_items', 'customers', 'products', 'sellers']
    for table in required:
        if table not in datasets or datasets[table].empty:
            raise ValueError(f"Dataset requerido '{table}' no disponible")
    
    # Base: orders (filtrar solo delivered)
    master = datasets['orders'].copy()
    if 'order_status' in master.columns:
        master = master[master['order_status'] == 'delivered'].copy()
        logger.info(f"   Base (orders delivered): {len(master):,} registros")
    else:
        logger.info(f"   Base (orders): {len(master):,} registros")
    
    # Join con customers
    if not datasets['customers'].empty:
        master = master.merge(
            datasets['customers'],
            on='customer_id',
            how='left',
            suffixes=('', '_cust')
        )
        logger.info(f"   + customers: {len(master):,} registros")
    
    # Join con order_items
    if not datasets['order_items'].empty:
        master = master.merge(
            datasets['order_items'],
            on='order_id',
            how='left',
            suffixes=('', '_item')
        )
        logger.info(f"   + order_items: {len(master):,} registros")
    
    # Join con products
    if not datasets['products'].empty:
        master = master.merge(
            datasets['products'],
            on='product_id',
            how='left',
            suffixes=('', '_prod')
        )
        logger.info(f"   + products: {len(master):,} registros")
    
    # Join con sellers
    if not datasets['sellers'].empty:
        master = master.merge(
            datasets['sellers'],
            on='seller_id',
            how='left',
            suffixes=('', '_sell')
        )
        logger.info(f"   + sellers: {len(master):,} registros")
    
    # Join con order_payments
    if not datasets['order_payments'].empty:
        master = master.merge(
            datasets['order_payments'],
            on='order_id',
            how='left',
            suffixes=('', '_pay')
        )
        logger.info(f"   + order_payments: {len(master):,} registros")
    
    # Join con order_reviews
    if not datasets['order_reviews'].empty:
        master = master.merge(
            datasets['order_reviews'],
            on='order_id',
            how='left',
            suffixes=('', '_rev')
        )
        logger.info(f"   + order_reviews: {len(master):,} registros")
    
    # Join con category_translation
    if not datasets['category_translation'].empty and 'product_category_name' in master.columns:
        master = master.merge(
            datasets['category_translation'],
            on='product_category_name',
            how='left',
            suffixes=('', '_trans')
        )
        logger.info(f"   + category_translation: {len(master):,} registros")
    
    # Nota: geolocation se puede usar despu√©s para calcular distancias
    # pero no se hace join directo porque tiene m√∫ltiples registros por zip
    
        # Nota: geolocation se puede usar despu√©s para calcular distancias
    # pero no se hace join directo porque tiene m√∫ltiples registros por zip
    
    # ============================================================
    # ELIMINAR DUPLICADOS POR ORDER_ID
    # ============================================================
    logger.info("   üßπ Verificando duplicados...")
    initial_count = len(master)
    
    # Contar duplicados por order_id
    duplicates_count = master.duplicated(subset=['order_id'], keep='first').sum()
    
    if duplicates_count > 0:
        logger.warning(f"      ‚ö†Ô∏è  Duplicados detectados: {duplicates_count:,} registros")
        logger.info(f"      ‚Ä¢ Causa: √ìrdenes con m√∫ltiples items/pagos/reviews")
        
        # Eliminar duplicados manteniendo el primer registro
        master = master.drop_duplicates(subset=['order_id'], keep='first')
        
        logger.info(f"      ‚úÖ Duplicados eliminados: {initial_count - len(master):,}")
        logger.info(f"      üìä Registros √∫nicos finales: {len(master):,}")
    else:
        logger.info(f"      ‚úÖ No se detectaron duplicados en order_id")
    
    # ============================================================
    # CALCULAR TARGET: Delayed_time
    # ============================================================
    if 'order_delivered_customer_date' in master.columns and 'order_estimated_delivery_date' in master.columns:
        logger.info("   üéØ Calculando target 'Delayed_time'...")
        
        # Convertir a datetime
        master['order_delivered_customer_date'] = pd.to_datetime(master['order_delivered_customer_date'])
        master['order_estimated_delivery_date'] = pd.to_datetime(master['order_estimated_delivery_date'])
        
        # Calcular diferencia en d√≠as
        master['Delayed_time'] = (
            master['order_delivered_customer_date'] - 
            master['order_estimated_delivery_date']
        ).dt.days
        
        logger.info(f"      ‚Ä¢ Rango: [{master['Delayed_time'].min():.0f}, {master['Delayed_time'].max():.0f}] d√≠as")
        logger.info(f"      ‚Ä¢ Media: {master['Delayed_time'].mean():.1f} d√≠as")
        logger.info(f"      ‚Ä¢ Valores nulos: {master['Delayed_time'].isna().sum():,}")
    
    # ============================================================
    # FILTRAR OUTLIERS
    # ============================================================
    initial_count = len(master)
    
    if 'Delayed_time' in master.columns:
        logger.info("   üßπ Filtrando outliers en Delayed_time...")
        
        # Filtrar rango v√°lido: -10 a +20 d√≠as
        master = master[
            (master['Delayed_time'] >= -30) & 
            (master['Delayed_time'] <= 60)
        ].copy()
        
        removed = initial_count - len(master)
        logger.info(f"      ‚Ä¢ Outliers removidos: {removed:,} registros ({removed/initial_count*100:.1f}%)")
        logger.info(f"      ‚Ä¢ Registros finales: {len(master):,}")
    
    logger.info(f"   ‚úÖ Master Table: {len(master):,} registros, {len(master.columns)} columnas")
    
    logger.info(f"   ‚úÖ Master Table: {len(master):,} registros, {len(master.columns)} columnas")
    
    return master

@task(log_prints=True)
def apply_one_hot_encoding(master_df: pd.DataFrame) -> pd.DataFrame:
    """
    Aplica One-Hot Encoding a las columnas categ√≥ricas de la master table.
    """
    logger = get_run_logger()
    logger.info("üî¢ Aplicando One-Hot Encoding a columnas categ√≥ricas...")
    
    initial_columns = len(master_df.columns)
    
    # Definir columnas categ√≥ricas para One-Hot Encoding
    categorical_columns = []
    
    # 1. Order status (si existe y no fue filtrado)
    if 'order_status' in master_df.columns:
        categorical_columns.append('order_status')
    
    # 2. Product category
    if 'product_category_name' in master_df.columns:
        categorical_columns.append('product_category_name')
    
    # 3. Payment type
    if 'payment_type' in master_df.columns:
        categorical_columns.append('payment_type')
    
    # 4. Customer state
    if 'customer_state' in master_df.columns:
        categorical_columns.append('customer_state')
    
    # 5. Seller state
    if 'seller_state' in master_df.columns:
        categorical_columns.append('seller_state')
    
    # 6. Customer city (CUIDADO: puede tener muchas categor√≠as)
    # Solo si tiene menos de 50 valores √∫nicos
    if 'customer_city' in master_df.columns:
        unique_cities = master_df['customer_city'].nunique()
        if unique_cities <= 50:
            categorical_columns.append('customer_city')
        else:
            logger.info(f"   ‚ö†Ô∏è  customer_city tiene {unique_cities} valores √∫nicos, se omite del encoding")
    
    # 7. Seller city (CUIDADO: puede tener muchas categor√≠as)
    if 'seller_city' in master_df.columns:
        unique_seller_cities = master_df['seller_city'].nunique()
        if unique_seller_cities <= 50:
            categorical_columns.append('seller_city')
        else:
            logger.info(f"   ‚ö†Ô∏è  seller_city tiene {unique_seller_cities} valores √∫nicos, se omite del encoding")
    
    # Filtrar solo las columnas que existen
    categorical_columns = [col for col in categorical_columns if col in master_df.columns]
    
    if not categorical_columns:
        logger.info("   ‚ÑπÔ∏è  No se encontraron columnas categ√≥ricas para encoding")
        return master_df
    
    logger.info(f"   üìã Columnas a encodear: {categorical_columns}")
    
    # Aplicar One-Hot Encoding
    for col in categorical_columns:
        # Contar valores √∫nicos
        unique_values = master_df[col].nunique()
        logger.info(f"      ‚Ä¢ {col}: {unique_values} categor√≠as √∫nicas")
        
        # Crear dummies
        dummies = pd.get_dummies(
            master_df[col], 
            prefix=col,
            drop_first=True,  # Evitar multicolinealidad
            dtype=int
        )
        
        # Agregar al dataframe
        master_df = pd.concat([master_df, dummies], axis=1)
        
        # Eliminar columna original
        master_df = master_df.drop(columns=[col])
        
        logger.info(f"         ‚úÖ Creadas {len(dummies.columns)} columnas dummy")
    
    final_columns = len(master_df.columns)
    new_columns = final_columns - initial_columns
    
    logger.info(f"   ‚úÖ One-Hot Encoding completado")
    logger.info(f"      ‚Ä¢ Columnas iniciales: {initial_columns}")
    logger.info(f"      ‚Ä¢ Columnas finales: {final_columns}")
    logger.info(f"      ‚Ä¢ Nuevas columnas: {new_columns}")
    
    return master_df

@task(log_prints=True)
def feature_selection(master_df: pd.DataFrame, target_col: str = 'Delayed_time', 
                     correlation_threshold: float = 0.05,
                     top_n_features: int = 50) -> pd.DataFrame:
    """
    Reduce variables mediante Feature Selection basado en el target.
    
    M√©todos utilizados:
    1. Correlaci√≥n de Pearson con el target
    2. Mutual Information
    3. Feature Importance de Random Forest
    
    Args:
        master_df: DataFrame con todas las features
        target_col: Nombre de la columna target
        correlation_threshold: Umbral m√≠nimo de correlaci√≥n absoluta
        top_n_features: N√∫mero m√°ximo de features a mantener
    
    Returns:
        DataFrame con features seleccionadas + target + IDs
    """
    logger = get_run_logger()
    logger.info("üéØ Aplicando Feature Selection...")
    
    if target_col not in master_df.columns:
        logger.warning(f"   ‚ö†Ô∏è  Target '{target_col}' no encontrado, se omite feature selection")
        return master_df
    
    # Separar columnas
    id_columns = [col for col in master_df.columns if 'id' in col.lower()]
    date_columns = [col for col in master_df.columns if master_df[col].dtype == 'datetime64[ns]']
    
    # Columnas num√©ricas (excluyendo IDs, fechas y target)
    numeric_cols = master_df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [col for col in numeric_cols 
                   if col != target_col 
                   and col not in id_columns]
    
    logger.info(f"   üìä An√°lisis inicial:")
    logger.info(f"      ‚Ä¢ Total columnas: {len(master_df.columns)}")
    logger.info(f"      ‚Ä¢ Columnas num√©ricas: {len(numeric_cols)}")
    logger.info(f"      ‚Ä¢ Features candidatas: {len(feature_cols)}")
    logger.info(f"      ‚Ä¢ IDs: {len(id_columns)}")
    logger.info(f"      ‚Ä¢ Fechas: {len(date_columns)}")
    
    if len(feature_cols) == 0:
        logger.warning("   ‚ö†Ô∏è  No hay features num√©ricas para seleccionar")
        return master_df
    
    # Preparar datos (eliminar NaN)
    X = master_df[feature_cols].fillna(0)
    y = master_df[target_col].fillna(0)
    
    # ============================================================
    # M√âTODO 1: Correlaci√≥n de Pearson
    # ============================================================
    logger.info("   üìà M√©todo 1: Correlaci√≥n de Pearson...")
    correlations = {}
    for col in feature_cols:
        corr = X[col].corr(y)
        correlations[col] = abs(corr)
    
    corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['correlation'])
    corr_df = corr_df.sort_values('correlation', ascending=False)
    
    # Features con correlaci√≥n significativa
    significant_corr = corr_df[corr_df['correlation'] >= correlation_threshold].index.tolist()
    logger.info(f"      ‚Ä¢ Features con |corr| >= {correlation_threshold}: {len(significant_corr)}")
    
    # ============================================================
    # M√âTODO 2: Mutual Information
    # ============================================================
    logger.info("   üîó M√©todo 2: Mutual Information...")
    mi_scores = mutual_info_regression(X, y, random_state=42)
    mi_df = pd.DataFrame({'feature': feature_cols, 'mi_score': mi_scores})
    mi_df = mi_df.sort_values('mi_score', ascending=False)
    
    # Top features por MI
    top_mi = mi_df.head(top_n_features)['feature'].tolist()
    logger.info(f"      ‚Ä¢ Top {min(top_n_features, len(feature_cols))} features por MI")
    
    # ============================================================
    # M√âTODO 3: Random Forest Feature Importance
    # ============================================================
    logger.info("   üå≤ M√©todo 3: Random Forest Feature Importance...")
    
    # Entrenar RF r√°pido (pocos √°rboles para velocidad)
    rf = RandomForestRegressor(
        n_estimators=50,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X, y)
    
    # Obtener importancias
    importances = rf.feature_importances_
    rf_df = pd.DataFrame({'feature': feature_cols, 'importance': importances})
    rf_df = rf_df.sort_values('importance', ascending=False)
    
    # Top features por RF
    top_rf = rf_df.head(top_n_features)['feature'].tolist()
    logger.info(f"      ‚Ä¢ Top {min(top_n_features, len(feature_cols))} features por RF")
    
    # ============================================================
    # COMBINAR M√âTODOS
    # ============================================================
    logger.info("   üîÄ Combinando m√©todos...")
    
    # Uni√≥n de features seleccionadas por los 3 m√©todos
    selected_features = list(set(significant_corr) | set(top_mi) | set(top_rf))
    
    # Limitar al top_n si es necesario
    if len(selected_features) > top_n_features:
        # Ordenar por promedio de rankings
        feature_scores = {}
        for feat in selected_features:
            corr_rank = list(corr_df.index).index(feat) if feat in corr_df.index else len(feature_cols)
            mi_rank = list(mi_df['feature']).index(feat) if feat in mi_df['feature'].values else len(feature_cols)
            rf_rank = list(rf_df['feature']).index(feat) if feat in rf_df['feature'].values else len(feature_cols)
            feature_scores[feat] = (corr_rank + mi_rank + rf_rank) / 3
        
        # Ordenar por score promedio
        sorted_features = sorted(feature_scores.items(), key=lambda x: x[1])
        selected_features = [f[0] for f in sorted_features[:top_n_features]]
    
    logger.info(f"   ‚úÖ Features seleccionadas: {len(selected_features)}")
    
    # ============================================================
    # CREAR DATAFRAME REDUCIDO
    # ============================================================
    # Mantener: IDs + features seleccionadas + target + fechas importantes
    important_dates = ['order_purchase_timestamp', 'order_delivered_customer_date', 
                      'order_estimated_delivery_date']
    keep_dates = [col for col in important_dates if col in master_df.columns]
    
    final_columns = id_columns + selected_features + [target_col] + keep_dates
    final_columns = [col for col in final_columns if col in master_df.columns]
    
    reduced_df = master_df[final_columns].copy()
    
    # ============================================================
    # RESUMEN
    # ============================================================
    logger.info("")
    logger.info("   üìä Resumen de Feature Selection:")
    logger.info(f"      ‚Ä¢ Columnas originales: {len(master_df.columns)}")
    logger.info(f"      ‚Ä¢ Columnas finales: {len(reduced_df.columns)}")
    logger.info(f"      ‚Ä¢ Reducci√≥n: {len(master_df.columns) - len(reduced_df.columns)} columnas ({(1 - len(reduced_df.columns)/len(master_df.columns))*100:.1f}%)")
    logger.info("")
    logger.info("   üèÜ Top 10 features por importancia combinada:")
    for i, feat in enumerate(selected_features[:10], 1):
        corr_val = correlations.get(feat, 0)
        logger.info(f"      {i}. {feat} (corr: {corr_val:.3f})")
    
    return reduced_df

@task(log_prints=True)
def optimize_xgboost_hyperparameters(
    X_train: pd.DataFrame, 
    y_train: pd.Series, 
    X_val: pd.DataFrame, 
    y_val: pd.Series,
    n_trials: int = 50
) -> Dict:
    """
    Optimiza hiperpar√°metros de XGBoost usando Optuna.
    
    Args:
        X_train: Features de entrenamiento
        y_train: Target de entrenamiento
        X_val: Features de validaci√≥n
        y_val: Target de validaci√≥n
        n_trials: N√∫mero de trials de Optuna
    
    Returns:
        Dict con mejores hiperpar√°metros
    """
    logger = get_run_logger()
    logger.info(f"üîç Optimizando hiperpar√°metros con Optuna ({n_trials} trials)...")
    
    def objective(trial):
        """Funci√≥n objetivo para Optuna."""
        # Definir espacio de b√∫squeda de hiperpar√°metros
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'gamma': trial.suggest_float('gamma', 0.0, 0.5),
            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': 0
        }
        
        # Entrenar modelo con estos hiperpar√°metros
        model = xgb.XGBRegressor(**params)
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
        
        # Predecir en validaci√≥n
        y_pred = model.predict(X_val)
        
        # Calcular MAE (m√©trica a minimizar)
        mae = mean_absolute_error(y_val, y_pred)
        
        return mae
    
    # Crear estudio de Optuna
    study = optuna.create_study(
        direction='minimize',  # Minimizar MAE
        sampler=TPESampler(seed=42),
        study_name='xgboost_optimization'
    )
    
    # Ejecutar optimizaci√≥n
    study.optimize(
        objective, 
        n_trials=n_trials,
        show_progress_bar=False,
        n_jobs=1  # Usar 1 job porque XGBoost ya usa todos los cores
    )
    
    # Mejores hiperpar√°metros
    best_params = study.best_params
    best_mae = study.best_value
    
    logger.info("")
    logger.info("   ‚úÖ Optimizaci√≥n completada")
    logger.info(f"      ‚Ä¢ Mejor MAE: {best_mae:.4f} d√≠as")
    logger.info(f"      ‚Ä¢ Trials completados: {len(study.trials)}")
    logger.info("")
    logger.info("   üèÜ Mejores hiperpar√°metros:")
    for param, value in best_params.items():
        logger.info(f"      ‚Ä¢ {param}: {value}")
    
    return {
        'best_params': best_params,
        'best_mae': best_mae,
        'study': study
    }

@task(log_prints=True)
def train_xgboost_model(
    master_df: pd.DataFrame, 
    target_col: str = 'Delayed_time',
    use_optuna: bool = True,
    n_trials: int = 50
) -> Dict:
    """
    Entrena un modelo XGBoost Regressor para predecir el target.
    Opcionalmente usa Optuna para optimizar hiperpar√°metros.
    
    Args:
        master_df: DataFrame con features y target
        target_col: Nombre de la columna target
        use_optuna: Si True, usa Optuna para optimizar hiperpar√°metros
        n_trials: N√∫mero de trials de Optuna
    
    Returns:
        Dict con master_df actualizado y m√©tricas del modelo
    """
    logger = get_run_logger()
    logger.info("ü§ñ Entrenando modelo XGBoost Regressor...")
    
    if target_col not in master_df.columns:
        logger.warning(f"   ‚ö†Ô∏è  Target '{target_col}' no encontrado, se omite entrenamiento")
        return {'master_df': master_df, 'metrics': None}
    
    # ============================================================
    # PREPARAR DATOS
    # ============================================================
    logger.info("   üìä Preparando datos...")
    
    # Identificar columnas
    id_columns = [col for col in master_df.columns if 'id' in col.lower()]
    date_columns = [col for col in master_df.columns if master_df[col].dtype == 'datetime64[ns]']
    
    # Features num√©ricas (excluyendo IDs, fechas y target)
    numeric_cols = master_df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [col for col in numeric_cols 
                   if col != target_col 
                   and col not in id_columns]
    
    logger.info(f"      ‚Ä¢ Features para entrenamiento: {len(feature_cols)}")
    logger.info(f"      ‚Ä¢ Registros totales: {len(master_df):,}")
    
    # Preparar X e y
    X = master_df[feature_cols].fillna(0)
    y = master_df[target_col].fillna(0)
    
    # Verificar que hay datos suficientes
    if len(X) < 100:
        logger.warning(f"   ‚ö†Ô∏è  Datos insuficientes para entrenamiento ({len(X)} registros)")
        return {'master_df': master_df, 'metrics': None}
    
    # ============================================================
    # SPLIT TRAIN/VALIDATION/TEST
    # ============================================================
    logger.info("   ‚úÇÔ∏è  Dividiendo datos (60% train, 20% val, 20% test)...")
    
    # Primero: train+val (80%) vs test (20%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # Segundo: train (75% de 80% = 60%) vs val (25% de 80% = 20%)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42
    )
    
    logger.info(f"      ‚Ä¢ Train: {len(X_train):,} registros")
    logger.info(f"      ‚Ä¢ Validation: {len(X_val):,} registros")
    logger.info(f"      ‚Ä¢ Test: {len(X_test):,} registros")
    
    # ============================================================
    # OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS CON OPTUNA
    # ============================================================
    if use_optuna:
        optuna_result = optimize_xgboost_hyperparameters(
            X_train, y_train, 
            X_val, y_val,
            n_trials=n_trials
        )
        best_params = optuna_result['best_params']
        best_params['random_state'] = 42
        best_params['n_jobs'] = -1
        best_params['verbosity'] = 0
    else:
        # Hiperpar√°metros por defecto
        logger.info("   ‚öôÔ∏è  Usando hiperpar√°metros por defecto...")
        best_params = {
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': 0
        }
    
    # ============================================================
    # ENTRENAR MODELO FINAL
    # ============================================================
    logger.info("")
    logger.info("   üöÄ Entrenando modelo final con mejores hiperpar√°metros...")
    
    model = xgb.XGBRegressor(**best_params)
    
    # Entrenar con train + validation
    X_train_full = pd.concat([X_train, X_val])
    y_train_full = pd.concat([y_train, y_val])
    
    model.fit(
        X_train_full, y_train_full,
        eval_set=[(X_test, y_test)],
        verbose=False
    )
    
    logger.info(f"      ‚úÖ Modelo entrenado")
    
    # ============================================================
    # EVALUAR MODELO
    # ============================================================
    logger.info("")
    logger.info("   üìà Evaluando modelo...")
    
    # Predicciones
    y_train_pred = model.predict(X_train_full)
    y_test_pred = model.predict(X_test)
    
    # M√©tricas en train
    train_mae = mean_absolute_error(y_train_full, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train_full, y_train_pred))
    train_r2 = r2_score(y_train_full, y_train_pred)
    
    # M√©tricas en test
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_r2 = r2_score(y_test, y_test_pred)
    
    logger.info("")
    logger.info("   üìä M√©tricas del Modelo:")
    logger.info("      TRAIN:")
    logger.info(f"         ‚Ä¢ MAE:  {train_mae:.2f} d√≠as")
    logger.info(f"         ‚Ä¢ RMSE: {train_rmse:.2f} d√≠as")
    logger.info(f"         ‚Ä¢ R¬≤:   {train_r2:.4f}")
    logger.info("      TEST:")
    logger.info(f"         ‚Ä¢ MAE:  {test_mae:.2f} d√≠as")
    logger.info(f"         ‚Ä¢ RMSE: {test_rmse:.2f} d√≠as")
    logger.info(f"         ‚Ä¢ R¬≤:   {test_r2:.4f}")
    
    # ============================================================
    # GENERAR PREDICCIONES PARA TODO EL DATASET
    # ============================================================
    logger.info("")
    logger.info("   üîÆ Generando predicciones para todo el dataset...")
    
    predictions = model.predict(X)
    master_df['Delayed_time_predicted'] = predictions
    
    # Calcular error de predicci√≥n
    master_df['prediction_error'] = master_df[target_col] - master_df['Delayed_time_predicted']
    master_df['prediction_error_abs'] = abs(master_df['prediction_error'])
    
    logger.info(f"      ‚úÖ Predicciones agregadas como 'Delayed_time_predicted'")
    logger.info(f"      ‚Ä¢ Error promedio: {master_df['prediction_error_abs'].mean():.2f} d√≠as")
    logger.info(f"      ‚Ä¢ Error mediano: {master_df['prediction_error_abs'].median():.2f} d√≠as")
    
    # ============================================================
    # FEATURE IMPORTANCE
    # ============================================================
    logger.info("")
    logger.info("   üèÜ Top 10 Features m√°s importantes:")
    
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for i, row in feature_importance.head(10).iterrows():
        logger.info(f"      {i+1}. {row['feature']}: {row['importance']:.4f}")
    
    # ============================================================
    # RETORNAR RESULTADOS
    # ============================================================
    metrics = {
        'train': {
            'mae': float(train_mae),
            'rmse': float(train_rmse),
            'r2': float(train_r2)
        },
        'test': {
            'mae': float(test_mae),
            'rmse': float(test_rmse),
            'r2': float(test_r2)
        },
        'n_features': len(feature_cols),
        'best_params': best_params if use_optuna else None
    }
    
    return {
        'master_df': master_df,
        'metrics': metrics,
        'model': model,
        'feature_cols': feature_cols
    }

@task(log_prints=True)
def save_model_pickle(
    model,
    model_metrics: Dict,
    feature_cols: List[str],
    output_path: str = '/workspace/xgboost_model_final.pkl'
) -> Dict:
    """
    Guarda el modelo entrenado en formato pickle.
    
    Args:
        model: Modelo entrenado
        model_metrics: M√©tricas del modelo
        feature_cols: Lista de features utilizadas
        output_path: Ruta donde guardar el modelo
    
    Returns:
        Dict con informaci√≥n del guardado
    """
    logger = get_run_logger()
    logger.info("üíæ Guardando modelo en formato pickle...")
    
    # Crear diccionario con modelo y metadata
    model_package = {
        'model': model,
        'metrics': model_metrics,
        'feature_columns': feature_cols,
        'model_type': type(model).__name__,
        'timestamp': pd.Timestamp.now().isoformat()
    }
    
    # Guardar en pickle
    with open(output_path, 'wb') as f:
        pickle.dump(model_package, f)
    
    # Verificar tama√±o del archivo
    import os
    file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
    
    logger.info(f"   ‚úÖ Modelo guardado exitosamente")
    logger.info(f"   üìÅ Ruta: {output_path}")
    logger.info(f"   üìä Tama√±o: {file_size:.2f} MB")
    logger.info(f"   üéØ Features: {len(feature_cols)}")
    logger.info(f"   üìà Test MAE: {model_metrics['test']['mae']:.3f} d√≠as")
    logger.info(f"   üìà Test R¬≤: {model_metrics['test']['r2']:.4f}")
    
    return {
        'output_path': output_path,
        'file_size_mb': file_size,
        'n_features': len(feature_cols)
    }

@task(log_prints=True)
def save_master_table(master_df: pd.DataFrame, datasets: Dict[str, pd.DataFrame]) -> Dict:
    """Guarda la master table y geolocation en Gold."""
    logger = get_run_logger()
    logger.info("üíæ Guardando Master Table en Gold...")
    
    # Guardar master table
    master_df.to_sql(
        'master_table',
        engine_gold,
        schema='dm',
        if_exists='replace',
        index=False,
        method='multi',
        chunksize=5000
    )
    
    logger.info(f"   ‚úÖ Tabla 'gold.dm.master_table' creada")
    logger.info(f"   üìä {len(master_df):,} registros, {len(master_df.columns)} columnas")
    
    # Guardar geolocation por separado (para c√°lculos de distancia)
    if 'geolocation' in datasets and not datasets['geolocation'].empty:
        logger.info("   üó∫Ô∏è  Procesando geolocation (eliminando duplicados)...")
        geo_df = datasets['geolocation'].copy()
        
        # Convertir zip code a num√©rico
        geo_df['geolocation_zip_code_prefix'] = pd.to_numeric(
            geo_df['geolocation_zip_code_prefix'], 
            errors='coerce'
        )
        
        # Eliminar duplicados agrupando por zip code y promediando coordenadas
        geo_unique = geo_df.groupby('geolocation_zip_code_prefix').agg({
            'geolocation_lat': 'mean',
            'geolocation_lng': 'mean',
            'geolocation_city': 'first',
            'geolocation_state': 'first'
        }).reset_index()
        
        initial_count = len(geo_df)
        final_count = len(geo_unique)
        duplicates_removed = initial_count - final_count
        
        logger.info(f"      ‚Ä¢ Registros iniciales: {initial_count:,}")
        logger.info(f"      ‚Ä¢ Duplicados eliminados: {duplicates_removed:,}")
        logger.info(f"      ‚Ä¢ Registros √∫nicos: {final_count:,}")
        
        # Guardar geolocation sin duplicados
        geo_unique.to_sql(
            'geolocation',
            engine_gold,
            schema='dm',
            if_exists='replace',
            index=False,
            method='multi',
            chunksize=5000
        )
        logger.info(f"   ‚úÖ Tabla 'gold.dm.geolocation' creada ({final_count:,} registros √∫nicos)")
    
    return {
        "status": "success",
        "total_rows": len(master_df),
        "total_columns": len(master_df.columns),
        "target_stats": {
            "min": float(master_df['Delayed_time'].min()) if 'Delayed_time' in master_df.columns else None,
            "max": float(master_df['Delayed_time'].max()) if 'Delayed_time' in master_df.columns else None,
            "mean": float(master_df['Delayed_time'].mean()) if 'Delayed_time' in master_df.columns else None
        }
    }


@flow(name="Silver ‚Üí Gold (Master Table + Features)", log_prints=True)
def silver_to_gold():
    """
    Flujo que construye la master table y features completas en Gold
    desde todas las tablas de Silver
    """
    logger = get_run_logger()
    logger.info("=" * 80)
    logger.info("ü•á INICIANDO CONSTRUCCI√ìN DE MASTER TABLE + FEATURES - GOLD")
    logger.info("=" * 80)
    
    # 1. Cargar datasets desde Silver
    datasets = load_datasets_from_silver()
    
    # 2. Construir master table base
    master_df = build_master_table(datasets)
    
    # 2.5. Aplicar One-Hot Encoding
    master_df = apply_one_hot_encoding(master_df)
    
        # 2.6. Feature Selection (reducir variables)
    master_df = feature_selection(
        master_df, 
        target_col='Delayed_time',
        correlation_threshold=0.05,
        top_n_features=50
    )

    # ============================================================
    # EVALUACI√ìN CON VALIDACI√ìN CRUZADA
    # ============================================================
    logger.info("")
    logger.info("=" * 80)
    logger.info("üî¨ EVALUACI√ìN DE MODELOS CON VALIDACI√ìN CRUZADA")
    logger.info("=" * 80)
    
    # Evaluar m√∫ltiples modelos con CV
    cv_results = evaluate_models_with_cv(
        master_df,
        target_col='Delayed_time',
        cv_folds=5,
        save_results=True
    )
    
    best_model_name = cv_results['best_model']
    logger.info(f"")
    logger.info(f"üèÜ Mejor modelo identificado: {best_model_name}")
    logger.info(f"   ‚Ä¢ Test MAE promedio: {cv_results['results_df'].iloc[0]['test_mae_mean']:.3f} d√≠as")
    logger.info(f"   ‚Ä¢ Features utilizadas: {cv_results['n_features']}")
    logger.info(f"")
    
    # 2.7. Entrenar modelo XGBoost con Optuna y generar predicciones
    xgb_result = train_xgboost_model(
        master_df, 
        target_col='Delayed_time',
        use_optuna=True,  # Activar optimizaci√≥n con Optuna
        n_trials=50  # N√∫mero de trials (ajustable: 30-100)
    )
    master_df = xgb_result['master_df']
    model_metrics = xgb_result['metrics']

    # ============================================================
    # GUARDAR MODELO EN PICKLE
    # ============================================================
    logger.info("")
    logger.info("=" * 80)
    logger.info("üíæ GUARDANDO MODELO FINAL")
    logger.info("=" * 80)
    
    save_result = save_model_pickle(
        model=xgb_result['model'],
        model_metrics=model_metrics,
        feature_cols=xgb_result['feature_cols'],
        output_path='/workspace/xgboost_model_final.pkl'
    )
    
    # 3. Guardar master_table en Gold (ahora con predicciones)
    result = save_master_table(master_df, datasets)
    
    # ============================================================
    # GENERAR FEATURES COMPLETAS
    # ============================================================
    logger.info("")
    logger.info("=" * 80)
    logger.info("üéØ GENERANDO FEATURES COMPLETAS")
    logger.info("=" * 80)
    
    # 4. Generar features temporales
    logger.info("üìÖ Generando features temporales...")
    temporal_df = generate_temporal_features(master_df)
    
    # 5. Generar features log√≠sticas
    logger.info("üì¶ Generando features log√≠sticas...")
    logistics_df = generate_logistics_features(datasets)
    
    # 6. Generar features de pago
    logger.info("üí≥ Generando features de pago...")
    payment_df = generate_payment_features(datasets)
    
    # 7. Generar features de cliente
    logger.info("üë• Generando features de cliente...")
    customer_df = generate_customer_features(datasets)
    
    # 8. Combinar todas las features
    logger.info("üîó Combinando todas las features...")
    features_df = combine_features(temporal_df, logistics_df, payment_df, customer_df)
    
    # 9. Guardar features completas en Gold
    logger.info("üíæ Guardando features completas...")
    features_df.to_sql(
        'features',
        engine_gold,
        schema='dm',
        if_exists='replace',
        index=False,
        method='multi',
        chunksize=5000
    )
    logger.info(f"   ‚úÖ Tabla 'gold.dm.features' creada")
    logger.info(f"   üìä {len(features_df):,} registros, {len(features_df.columns)} columnas")
    
    # ============================================================
    # RESUMEN FINAL
    # ============================================================
    logger.info("")
    logger.info("=" * 80)
    logger.info("‚úÖ GOLD LAYER COMPLETADO")
    logger.info("=" * 80)
    logger.info(f"üìä Resumen:")
    logger.info(f"")
    logger.info(f"   MASTER TABLE:")
    logger.info(f"      ‚Ä¢ Registros: {result['total_rows']:,}")
    logger.info(f"      ‚Ä¢ Columnas: {result['total_columns']}")
    if result.get('target_stats'):
        logger.info(f"      ‚Ä¢ Target 'Delayed_time':")
        logger.info(f"         - Min: {result['target_stats']['min']:.0f} d√≠as")
        logger.info(f"         - Max: {result['target_stats']['max']:.0f} d√≠as")
        logger.info(f"         - Media: {result['target_stats']['mean']:.1f} d√≠as")
    
    # Agregar m√©tricas del modelo
    if model_metrics:
        logger.info(f"")
        logger.info(f"   MODELO XGBOOST:")
        logger.info(f"      ‚Ä¢ Test MAE: {model_metrics['test']['mae']:.2f} d√≠as")
        logger.info(f"      ‚Ä¢ Test RMSE: {model_metrics['test']['rmse']:.2f} d√≠as")
        logger.info(f"      ‚Ä¢ Test R¬≤: {model_metrics['test']['r2']:.4f}")
        logger.info(f"      ‚Ä¢ Features usadas: {model_metrics['n_features']}")
    logger.info(f"")
    logger.info(f"   FEATURES TABLE:")
    logger.info(f"      ‚Ä¢ Registros: {len(features_df):,}")
    logger.info(f"      ‚Ä¢ Columnas: {len(features_df.columns)}")
    logger.info(f"")
    logger.info(f"   GEOLOCATION:")
    logger.info(f"      ‚Ä¢ Tabla sin duplicados guardada")
    logger.info("=" * 80)
    
    # Actualizar resultado con info de features
    result['features'] = {
        'total_rows': len(features_df),
        'total_columns': len(features_df.columns)
    }
    
    return result


if __name__ == "__main__":
    silver_to_gold()